# Features (unorganized)

## Cluster creation

### valkey.conf

Cluster settings
```ini
cluster-enabled yes
cluster-require-full-coverage no
cluster-node-timeout 10000
cluster-config-file /data/nodes.conf
cluster-migration-barrier 1
```

TLS settings
```ini
tls-port 6379
port 0
tls-cert-file /etc/valkey-cluster/tls/server.crt
tls-key-file /etc/valkey-cluster/tls/server.key
tls-ca-cert-file /etc/valkey-cluster/tls/ca_bundle.pem
tls-auth-clients yes
tls-cluster yes
tls-replication yes
```

```sh
valkey-cli --cluster create --cluster-replicas 1 node-0.ns:6379 ... node-n.ns:6379
```

Pod Disruption Budget
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: valkey-poc
spec:
  minAvailable: N-1
  selector:
    matchLabels:
      app: valkey-poc
```

## CR Changes Reconciliation
TBD

## **Operator** recovery and resources reconciliation from restart/scale 0
TBD

## Resources identification

Mark all created kubernetes resources (statefulsets, configmaps, secrets, pvcs, svcs, pdbs etc) with an annotation, to clearly link them to the `ValkeyCluster`. Like
```yaml
metadata:
  annotations:
    valkey.io/cr: valkey-poc    # name of the ValkeyCluster
    valkey.io/locked: "true"    # maybe...
  ownerReferences:
    # etc
```

All created CRs must have set correct `ownerReferences`.

## Unbound Valkey cluster from **operator**

If the user wishes to take control over Valkey cluster, it will be allowed to remove `valkey.io/cr: valkey-poc` from `statefulset`. After doing this, the operator will perform the following:
- clear all other annotations from resources
- will clear all statuses from `ValkeyClusterStatus` and will change to
```yaml
status:
  phase: Unbound
```
**TBD:** what to do and how, if user wants it back

## Validation Webhooks

In order to distinguish changes self vs others, **operator** generates a unique token per update. The webhook maintains a short-lived cache of valid tokens. Once used, the token is invalidated.

```yaml
metadata:
  annotations:
    valkey.io/update-token: "abc123"
```

All other changes are rejected. Exceptions are pods, for `delete`.

## Operator - Sidecar communication

**Operator** is mostly idle, the **sidecarâ€™s** heartbeat is low-frequency and lightweight. The communication will be
- **Sidecar** initiated - Heartbits or major events: HTTP/1.1. Major events can be Valkey failed, someone deleted the pod
- **Operator** initiated - rest of communication: gRPC - only **orchestrator** open channels to needed **sidecars** when orchestration is triggered. Even use short-lived gRPC streams if needed.

## TLS

If `externalCerts` is provided in `ValkeyClusterTls`, it will be validated.  
If not, using the one in `ValkeyCertificateBundle`, there will be generated certs with validity from `clientCertValidity`.  
In any case, they will be stored in secret from `tlsSecretName` and mounted in pods.

If the certs are provided, on change, they will be validated.  
If certs are autogenerated, there will be a cron job that will regenerate the certs on demand.  
In any case, they will be updated in secret from `tlsSecretName` and mounted in pods.

When the secret is changed, the pods must be restarted using _**Operator** Rolling Restarts_ (described also in this doc). If CA is the same, nothing must be done, else, a bundle must be provided.

TBD - status in CRs

## mTLS

**Operator** generates a root CA on install, saved in ValkeyCA. For each Valkey cluster, it generates an intermediate CA, saved in `ValkeyCertificateBundle`. The mTLS certificates are mounted from secrets in containers as volumes. Maybe must mount CA in containers in ca-bundle (in valkey no, in sidecar maybe). On cert change, **Sidecar** will reload and use it. **Operator** will accept a period of time (1m? 1h?) the older cert. After all sidecars are switched, will unload it from memory and *(TBD)* will clean ValkeyCertificateBinding of deprecated.

**AuthN/AuthZ:** use SPIFFE-style SANs to authorize requests (e.g. only sidecars with CN `valkey-poc-sidecar` can respond).

Protobuf contracts with versioning.

TBD - status in CRs

## Rolling restarts

When an **operator** rolling restart is triggered, the **operator** doesn't simulate a kubernetes one  ans in it will not restart pods from last to first, but will first restart all replicas (slaves), then will switch all primaries to replicas, and then will restart previous primaries (now replicas).

> **Important:** all actions are taking place only if the cluster is healthy. The check is before and after all (sub)phases

**Operator** loads cluster state from CR. It is mainly to get the list of primaries and replicas.

Phase 1 - replicas restart  

Subphase 1 - preparation - all steps are in paralel:
1. **Operator** opens a channel to **sidecar** in replica pod
2. **Operator** announces the intent of delete
3. **Sidecar** makes a last verification that is a replica and aknoledges. If not, the entire process is restarted.
4. **Sidecar** will not block/delay the restart (possible usage of `lifecycle: prestop:`)

Subpahse 2 - aknowledges await - here **operator** waits for all replicas' okays.  

Subphase 3 - restarts - all steps are sequential (*TBD* should they be in paralel? I do not see why not. if so, take care also of the `PDB`):
1. **Operator** removes `finalizers: - valkey.io/finalize`
2. **Operator** deletes the pod
3. Upon recreation of pod, **operator** applies back `finalizers: - valkey.io/finalize`
3. **Operator** waits for the pods to be online + sidecar
4. *TBD* what to do with DNS TTL issue. should it wait after the last one the duration of TTL?

Phase 2 - primaries/replicas switch - all steps are sequential. this is a moment of disrupture
1. **Operator** chooses a replica and opens a channel
2. **Operator** asks **sidecar** to perform `CLUSTER FAILOVER`
3. **Operator** waits for new status from **sidecar**

Phase 3 - old primaries (now replicas) restart  
Same steps from Phase 1

### User triggered via command/api/etc

When user runs `kubectl rollout restart statefulset` (or eq), kubernetes sets a new `spec.template.metadata.annotations["kubectl.kubernetes.io/restartedAt"]` timestamp on the StatefulSet. Normally, that change causes the StatefulSet controller to recreate pods; due to `updateStrategy: type: OnDelete`, nothing will happen.

**Operator** will detect this change and will trigger a rollout restart.

TBD - mainly it is Pod deletion blocking for all, one by one

## Pods deletion blocking

TBD - to make it more clear, also to explain what to implement and how (from ways of blocking). sigterm trap for sidecar is missing (do we need this if valkey-preStop.sh exists?). and operator promoting primaries. and DNS issue with headless svc. and status in CRs is missing

In order to block/delay pod deletion, the following will be implemented:
| What | Where | Comments |
|------|-------|----------|
| `finalizers: - valkey.io/finalizer` | Pod | _TBD_ do we really need this? it appeared as a mean of blocking rolling restarts/updates |
| `updateStrategy: type: OnDelete` | Statefulset | it will make manual pod delete mandatory in order to change anything |
| `lifecycle: preStop: exec: ` | Containers | on all of them. Upon pod delete, the `valkey-preStop.sh` will be executed |

The `valkey-preStop.sh` file is watched for by **sidecar**
- if there were no messages from **operator**, the **sidecar** will signal back to operator that an unexpected delete is taking place. If primary node, the **operator** will apply Phase 2 from _Rolling restarts_, and will signal **sidecar** to continue with pod delete, **sidecar** will delete the file `/tmp/pod-deleted-signal.txt`
- if it is a planned delete, the sidecar will just delete the `/tmp/pod-deleted-signal.txt` file

### valkey-preStop.sh script
```sh
#!/bin/sh

# Create the signal file
touch /tmp/pod-deleted-signal.txt
echo "Waiting for /tmp/pod-deleted-signal.txt to be deleted..."

# Loop until the file is gone
while [ -f /tmp/pod-deleted-signal.txt ]; do
  sleep 1
done

echo "Signal file deleted. Exiting."
```

## ACL creation and rotation

### Creation
Store pregenerated ACL file as a Kubernetes Secret

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: valkey-poc-acl
type: Opaque
stringData:
  users.acl: |
    user admin on >StrongPassword +@all ~*
```

Mount it as a read-only volume:
```yaml
# container spec
volumeMounts:
- name: users-acl-volume
  mountPath: /etc/valkey/acl
  readOnly: true
# ...
# pod spec
volumes:
- name: users-acl-volume
  secret:
    secretName: valkey-poc-acl
    items:
    - key: users.acl
      path: users.acl
```
Reference it in `valkey.conf`:
```ini
aclfile /etc/valkey/acl/users.acl
```

### Secret rotation
When a change in `ValkeyClusterAcl` or in secret is detected and reconciled, the following happens:
1. **Operator** creates the new secret with the rendered users.acl
2. **Operator** opens grpc with all sidecars
3. **Operator** confirms with all sidecars that ACL file is ok (do we need to test k8s here?)
4. **Operator** asks for acl reapply
5. **Sidecar** checks if its password changed
6. **Sidecar** connects to Valkey and executes
```resp
ACL LOAD
```
7. **Sidecar** (if password changed, reconnects and) checks ACL
```resp
ACL LIST
```
8. **Sidecar** reports back status
9. **Operator** change status in `ValkeyClusterAcl`

## Deferred executions

There are cases when one object depends on another. Use Indexing for Reverse Lookup: with [controller-runtime](https://pkg.go.dev/sigs.k8s.io/controller-runtime), index the CRs by the Secrets they reference:

```go
mgr.GetFieldIndexer().IndexField(&ValkeyClusterAcl{}, "spec.secrets.secretName", func(obj client.Object) []string {
    return []string{obj.(*ValkeyClusterAcl).Spec.Secrets.SecretName}
})
```
Then, when a Secret changes, list all CRs that reference it:

```go
var aclList ValkeyClusterAclList
r.List(ctx, &aclList, client.MatchingFields{"spec.secrets.secretName": secret.Name})
```

The reconcile loop should:
- check if the Secret exists and is complete
- if not, set `status.phase`: `WaitingForSecret` and requeue
- if yes, proceed with work and update `status.phase`: `Applied`

Practicly, we have the following rules
1. When a CR changes, reconcile it
2. When a Secret changes, reconcile all CRs that reference it
3. Use Status to Track Progress
```yaml
status:
  phase: WaitingForSecret
  lastChecked: 2025-09-07T19:17:00Z
  missingKeys:
    - valkey-user-pass
```

Actual Known Deps:
| Who | DependsOn | Why/What |
|-----|-----------|----------|
| ValkeyCluster | ValkeyCertificateBundle | Intermediate CA for mTLS (and TLS) |
| ValkeyCluster | ValkeyCertificateBinding | Certs for mTLS (and TLS) |
| ValkeyCluster | ValkeyClusterAcl | Certs for mTLS (and TLS) |
| ValkeyCertificateBundle | ValkeyCertificateCA | Operator CA |
| ValkeyCertificateBundle | Secret | the secret that holds the intermediate certs |
| ValkeyCertificateBinding | ValkeyCertificateBundle | the issuer for certs |
| ValkeyCertificateBinding | Secrets | the secrets that hold the certs for mTLS (and TLS) |
| ValkeyClusterAcl | Secret | the secret that holds the ACL definitions |

## Others

**Status in CRs** - to reflect sidecar executions/Valkey cluster states  
**CRDs versioning** - as much as possible, do not change anything, add (with defaults)  
**Observability** - metrics (Prometheus), logging (structured)  
**Valkey /data** - accesible also from sidecar  
**Valkey /tmp** - common between containers

